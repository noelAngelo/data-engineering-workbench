{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3821b8c9-cb57-4ab8-a52f-f921f396a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9249dca4-0296-4e99-a03a-4be552e8bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINIO\n",
    "MINIO_ACCESS_KEY = 'hackme'\n",
    "MINIO_SECRET_KEY = 'becauseiforgottochangethepassword'\n",
    "MINIO_ENDPOINT = 'http://localhost:9000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58d2fbe4-1b72-4667-aa46-005b8935ed28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 02:41:45 WARN Utils: Your hostname, Noels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.14 instead (on interface en0)\n",
      "23/01/30 02:41:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/30 02:41:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/30 06:34:51 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 984738 ms exceeds timeout 120000 ms\n",
      "23/01/30 06:34:51 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9535319-3182-49b0-9f29-0160363bc792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f92ffeed-db82-45da-8408-a19c6d8c374a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create a table\n",
    "\n",
    "To create a Delta table, write a DataFrame out in the delta format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814ac2c-8308-415d-b13e-d8020657649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "data.write.format(\"delta\").save(\"s3a://delta-lake/demo1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c04f1-a778-4414-9637-4f0d893be34e",
   "metadata": {},
   "source": [
    "These operations create a new Delta table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta table, see Create a table and Write to a table.\n",
    "\n",
    "> Note: This quickstart uses local paths for Delta table locations. For configuring HDFS or cloud storage for Delta tables, see Storage configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ec023-f671-4daa-9a11-5d2cc7dcc2bd",
   "metadata": {},
   "source": [
    "# Read data\n",
    "\n",
    "\n",
    "You read data in your Delta table by specifying the path to the files: \"/tmp/delta-table\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642dc20-2c4c-4c2f-a959-b1b931424c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c46b0-9118-4339-8694-06e31e25530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pandas_on_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046fc020-0dc5-4fd3-8b18-996a474d9342",
   "metadata": {},
   "source": [
    "# Update table data\n",
    "\n",
    "Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5820e4-152e-4cec-ac4b-0c46b906170e",
   "metadata": {},
   "source": [
    "## Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e16824-658f-4415-a271-f2337987fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(5, 10)\n",
    "data.write.format(\"delta\").save(\"/tmp/delta-table\", source='parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbb204-388f-4988-b8ae-47037e69dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd3e4b-947d-4a68-98ee-7c7d56973c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
